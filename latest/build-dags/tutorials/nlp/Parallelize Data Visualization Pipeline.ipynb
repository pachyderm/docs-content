{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c080e819-dbb6-40a3-89b7-29482b1d27ff",
   "metadata": {},
   "source": [
    "# Parallelize Data Visualization Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615888f2-1835-4c08-843e-75c83762cba7",
   "metadata": {},
   "source": [
    "Currently, the data visualization pipeline only generates visuals for the training split of our dataset.\n",
    "\n",
    "In this notebook, we will:\n",
    "- Set up a \"local\" development environment with mount. \n",
    "- Test the current functionality of the visualization pipeline. \n",
    "- Update our pipeline to visualize all 3 data splits (train, val, and test). \n",
    "- Paralleize our pipeline to create these visualizations simultaneously. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdde1d3-c95a-48a0-906e-f6d722b5d4d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0. Install Requirements\n",
    "This notebook should be run after first deploying the market-sentiment example. \n",
    "\n",
    "The first thing that we need to do is install the requirements for our project in our notebook. We can do this by installing the `pip`requirements. \n",
    "\n",
    "Note: Best practice is to install these with a virtual env like `venv`, `pipenv` or `conda`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8fce5b-9cdc-481a-8022-fe815ebda534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eea5879-d235-4d40-b601-21c5dbdeea36",
   "metadata": {},
   "source": [
    "## 1. Mount Visualization pipeline data\n",
    "\n",
    "In a terminal (inside JupyterLab) run: \n",
    "```\n",
    "pachctl mount -r dataset@master -r sentiment_words@master /pfs/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a2176-62fd-4041-8062-57709af87dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not already installed, install the tree command to view the directory structure\n",
    "!sudo apt-get install tree -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab609091-1a1c-49d3-9fdc-30924d018dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/pfs\u001b[00m\n",
      "├── \u001b[01;34mdataset\u001b[00m\n",
      "│   ├── test.csv\n",
      "│   ├── train.csv\n",
      "│   └── validation.csv\n",
      "└── \u001b[01;34msentiment_words\u001b[00m\n",
      "    └── LoughranMcDonald_SentimentWordLists_2018.csv\n",
      "\n",
      "2 directories, 4 files\n"
     ]
    }
   ],
   "source": [
    "!tree /pfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101ab024-c11c-440d-b1ba-f585be951eae",
   "metadata": {},
   "source": [
    "## 2. Test current functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73eac84-647f-4b04-a544-806cd1e195c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7fbf7-4cfb-4873-ae6e-ed2efe572f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python data_visualization.py --data-file /pfs/dataset/train.csv --sentiment-words-file /pfs/sentiment_words/LoughranMcDonald_SentimentWordLists_2018.csv --output-dir ./output/ -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f85b53e-8b5b-44cb-ad1a-f12dad194e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34moutput/\u001b[00m\n",
      "├── correlation.png\n",
      "├── frequent_words.png\n",
      "└── word_cloud.png\n",
      "\n",
      "0 directories, 3 files\n"
     ]
    }
   ],
   "source": [
    "!tree output/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436f5da-6097-4a51-91a6-4bd4a7de192a",
   "metadata": {},
   "source": [
    "## 3. Modify the code to visualize each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1624fc5d-b7cf-41aa-ad8d-241fa79ae27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_visualization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_visualization.py\n",
    "# Python libraries\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "# Data Science modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from market_sentiment.nlp_utils import *\n",
    "from market_sentiment.data_utils import *\n",
    "from market_sentiment.visualize import visualize_frequent_words, generate_word_cloud\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "# Import Scikit-learn moduels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Sentiment Analysis Trainer\")\n",
    "parser.add_argument(\n",
    "    \"--data-dir\",\n",
    "    help=\"directory of dataset csv files.\",\n",
    "    default=\"/pfs/dataset/\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--sentiment-words-file\",\n",
    "    help=\"csv with sentiment word list\",\n",
    "    default=\"/pfs/sentiment_words/LoughranMcDonald_SentimentWordLists_2018.csv\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output-dir\", metavar=\"DIR\", default=\"/pfs/out\", help=\"output directory for model\"\n",
    ")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed value\")\n",
    "parser.add_argument(\n",
    "    \"-v\", \"--verbose\", help=\"increase output verbosity\", action=\"store_true\"\n",
    ")\n",
    "\n",
    "\n",
    "# Set Seaborn Style\n",
    "sns.set(style=\"white\", palette=\"deep\")\n",
    "\n",
    "\n",
    "def create_vis(filename, sentiment_words_file, output_dir, seed=42):\n",
    "    train_df = load_finphrase(filename)\n",
    "    file_basename = os.path.splitext(os.path.basename(filename))[0]\n",
    "\n",
    "    # Samples\n",
    "    pd.set_option(\"display.max_colwidth\", -1)\n",
    "    logging.debug(train_df.sample(n=1, random_state=seed))\n",
    "\n",
    "    # Encode the label\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_df[\"label\"])\n",
    "    train_df[\"label\"] = le.transform(train_df[\"label\"])\n",
    "    logging.debug(list(le.classes_))\n",
    "    logging.debug(train_df[\"label\"])\n",
    "\n",
    "    corpus = create_corpus(train_df)\n",
    "    fig = visualize_frequent_words(corpus, stop_words)\n",
    "    fig.savefig(os.path.join(output_dir, \"frequent_words_\" + file_basename + \".png\"))\n",
    "\n",
    "    wordcloud = generate_word_cloud(corpus, stop_words)\n",
    "    wordcloud.to_file(os.path.join(output_dir, \"word_cloud_\" + file_basename + \".png\"))\n",
    "\n",
    "    # Load sentiment data\n",
    "    sentiment_df = pd.read_csv(sentiment_words_file)\n",
    "\n",
    "    # Make all words lower case\n",
    "    sentiment_df[\"word\"] = sentiment_df[\"word\"].str.lower()\n",
    "    sentiments = sentiment_df[\"sentiment\"].unique()\n",
    "    sentiment_df.groupby(by=[\"sentiment\"]).count()\n",
    "\n",
    "    sentiment_dict = {\n",
    "        sentiment: sentiment_df.loc[sentiment_df[\"sentiment\"] == sentiment][\n",
    "            \"word\"\n",
    "        ].values.tolist()\n",
    "        for sentiment in sentiments\n",
    "    }\n",
    "\n",
    "    columns = [\n",
    "        \"tone_score\",\n",
    "        \"word_count\",\n",
    "        \"n_pos_words\",\n",
    "        \"n_neg_words\",\n",
    "        \"pos_words\",\n",
    "        \"neg_words\",\n",
    "    ]\n",
    "\n",
    "    # Analyze tone for original text dataframe\n",
    "    print(train_df.shape)\n",
    "    tone_lmdict = [\n",
    "        tone_count_with_negation_check(sentiment_dict, x.lower())\n",
    "        for x in tqdm(train_df[\"sentence\"], total=train_df.shape[0])\n",
    "    ]\n",
    "    tone_lmdict_df = pd.DataFrame(tone_lmdict, columns=columns)\n",
    "    train_tone_df = pd.concat(\n",
    "        [train_df, tone_lmdict_df.reindex(train_df.index)], axis=1\n",
    "    )\n",
    "    train_tone_df.head()\n",
    "\n",
    "    # Show corelations to next_decision\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    corr_columns = [\"label\", \"n_pos_words\", \"n_neg_words\"]\n",
    "    sns.heatmap(\n",
    "        train_tone_df[corr_columns].astype(float).corr(),\n",
    "        cmap=\"coolwarm\",\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "    )\n",
    "    plt.savefig(os.path.join(output_dir, \"correlation_\" + file_basename + \".png\"))\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parser.parse_args()\n",
    "    if args.verbose:\n",
    "        logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    # Set Random Seed\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    sentiment_words_file = args.sentiment_words_file\n",
    "    \n",
    "    for dirpath, dnames, fnames in os.walk(args.data_dir):\n",
    "        for f in fnames:\n",
    "            create_vis(os.path.join(dirpath, f), sentiment_words_file, args.output_dir, args.seed)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c973e3b-50aa-42e2-8764-c374f372b182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python data_visualization.py --data-dir /pfs/dataset/ --sentiment-words-file /pfs/sentiment_words/LoughranMcDonald_SentimentWordLists_2018.csv --output-dir ./output/ -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72190df7-97c9-4130-a421-6c6f7c48ad06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34moutput/\u001b[00m\n",
      "├── correlation_test.png\n",
      "├── correlation_train.png\n",
      "├── correlation_validation.png\n",
      "├── frequent_words_test.png\n",
      "├── frequent_words_train.png\n",
      "├── frequent_words_validation.png\n",
      "├── word_cloud_test.png\n",
      "├── word_cloud_train.png\n",
      "└── word_cloud_validation.png\n",
      "\n",
      "0 directories, 9 files\n"
     ]
    }
   ],
   "source": [
    "!tree output/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65fa631-fbf1-4fed-b28d-eab0c1ee74e9",
   "metadata": {},
   "source": [
    "## 4. Update Pipeline with our new version\n",
    "In this notebook, we will take a shortcut to avoid rebuilding a Docker image for the pipeline. This is useful for debugging, but proper a proper [development cycle](https://docs.pachyderm.com/latest/how-tos/developer-workflow/) is recommended in production to maintain reproducibility. \n",
    "\n",
    "<img src=\"https://docs.pachyderm.com/latest/assets/images/d_steps_analysis_pipeline.svg\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "Here we will inject our code as the entrypoint to our container by: \n",
    "1. Encoding our python file as base64\n",
    "2. Update the pipeline, injecting our new python file as the entrypoint. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b592731b-02fe-4954-926e-297c4d83c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def generate_command(python_file):\n",
    "    with open(python_file,'r') as file:\n",
    "        user_code = file.read()\n",
    "    code = user_code \n",
    "    encoded_code = base64.standard_b64encode(bytes(code, 'utf-8')).decode('ascii')\n",
    "    \n",
    "    command = r\"\"\"\n",
    "    python -c \"import base64; __name__ = \\\"__main__\\\"; exec(base64.b64decode(\\\"{code_64}\\\"))\"\n",
    "    \"\"\".strip((\"\\n\\r \")).format(code_64=encoded_code)\n",
    "    return command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01736769-dbb8-47cc-a870-be15f409a0a6",
   "metadata": {},
   "source": [
    "Using our Python client, [python-pachyderm](https://github.com/pachyderm/python-pachyderm/), we will update our pipeline. This will allow us to modify our pipeline programatically rather than modifying our pipeline definition file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "359d5033-8bc1-4fbd-b29c-4b28fae80e47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import python_pachyderm\n",
    "from python_pachyderm.service import pps_proto\n",
    "client = python_pachyderm.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0fa8602-0f00-492a-9dca-4098b95c6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_python_pipeline(name, client, command):\n",
    "    client.create_pipeline(\n",
    "        pipeline_name=name,\n",
    "        transform=python_pachyderm.service.pps_proto.Transform(\n",
    "            image=\"jimmywhitaker/market_sentiment:dev0.25\",\n",
    "            cmd=['/bin/sh'],\n",
    "            stdin=[command]\n",
    "        ),\n",
    "        input=pps_proto.Input(\n",
    "            cross=[\n",
    "                pps_proto.Input(pfs=pps_proto.PFSInput(repo=\"dataset\", glob=\"/\")),\n",
    "                pps_proto.Input(pfs=pps_proto.PFSInput(repo=\"sentiment_words\", glob=\"/\"))\n",
    "            ]),\n",
    "        update=True,\n",
    "        reprocess_spec=\"every_job\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2424fcb-a4c8-4058-98ae-5868d197b419",
   "metadata": {},
   "source": [
    "Update the pipeline with our base64 encoded python file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb4ba6a-64d6-46ae-a152-4c8838d650a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = generate_command('./data_visualization.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2a9db7b-90f4-4d53-aeff-861cbf1a0340",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_python_pipeline('visualizations', client, code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c37de8-dece-4dbd-85a2-92c9453d970d",
   "metadata": {},
   "source": [
    "After the job runs, we can view the output of our new pipeline and see we now have visualizations for each split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d699041-c819-4d17-b0ba-29a59ec5ea3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                           TYPE SIZE     \n",
      "/correlation_test.png          file 31.12KiB \n",
      "/correlation_train.png         file 30.94KiB \n",
      "/correlation_validation.png    file 31.26KiB \n",
      "/frequent_words_test.png       file 36.78KiB \n",
      "/frequent_words_train.png      file 37.15KiB \n",
      "/frequent_words_validation.png file 37.32KiB \n",
      "/word_cloud_test.png           file 63.49KiB \n",
      "/word_cloud_train.png          file 73.95KiB \n",
      "/word_cloud_validation.png     file 72.52KiB \n"
     ]
    }
   ],
   "source": [
    "!pachctl list file visualizations@master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2665db-f9dc-41aa-a590-feb38f830249",
   "metadata": {},
   "source": [
    "## 5. Parallelize Pachyderm Pipeline with Glob Pattern\n",
    "Pachyderm pipelines can easily parallelize your processing across your data with zero code changes. \n",
    "\n",
    "[Glob patterns](https://docs.pachyderm.com/latest/concepts/pipeline-concepts/datum/glob-pattern/) are the Pachyderm mechanism to do this. \n",
    "\n",
    "For example, a input with a glob `/` like so, \n",
    "```\n",
    "\"input\": \"dataset\"\n",
    "\"glob\": \"/\"\n",
    "```\n",
    "would pass all the files in the `dataset` repository to a job. \n",
    "\n",
    "\n",
    "In our example, that would be: \n",
    "```\n",
    "# Job 1 of 1\n",
    "/pfs/dataset\n",
    "     ├── test.csv\n",
    "     ├── train.csv\n",
    "     └── validation.csv\n",
    "```\n",
    "We can test this with the `pachctl glob file` command.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48e5207c-25e5-448f-b89c-dd6901a2dbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME TYPE SIZE     \n",
      "/    dir  669.1KiB \n"
     ]
    }
   ],
   "source": [
    "!pachctl glob file dataset@master:/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb0cbbc-e874-4a93-88fa-53deb914c1cb",
   "metadata": {},
   "source": [
    "However, if we change this glob pattern to `/*` then we tell Pachyderm to treat each file in the root directory of the repo as a separate datum, which means it should run as its own job. \n",
    "\n",
    "For example, an input with the glob `/*` like so, \n",
    "```\n",
    "\"input\": \"dataset\"\n",
    "\"glob\": \"/*\"\n",
    "```\n",
    "would pass one file from the `dataset` repository to  job. \n",
    "\n",
    "\n",
    "In our example, that would be: \n",
    "```\n",
    "# Job 1 of 3\n",
    "/pfs/dataset\n",
    "     └── test.csv\n",
    "     \n",
    "# Job 2 of 3\n",
    "/pfs/dataset\n",
    "     └── train.csv\n",
    "     \n",
    "# Job 3 of 3\n",
    "/pfs/dataset\n",
    "     └── validation.csv\n",
    "```\n",
    "\n",
    "This means that each file can be processed separately, and parallelized automatically by Pachyderm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ef3b27e-1c7c-415f-afd2-cc0e5523ddb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            TYPE SIZE     \n",
      "/test.csv       file 134.9KiB \n",
      "/train.csv      file 481.5KiB \n",
      "/validation.csv file 52.74KiB \n"
     ]
    }
   ],
   "source": [
    "!pachctl glob file dataset@master:/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6beda502-2c13-44c8-ae20-62c7e01365eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parallelized_python_pipeline(name, client, command):\n",
    "    client.create_pipeline(\n",
    "        pipeline_name=name,\n",
    "        transform=python_pachyderm.service.pps_proto.Transform(\n",
    "            image=\"jimmywhitaker/market_sentiment:dev0.25\",\n",
    "            cmd=['/bin/sh'],\n",
    "            stdin=[command]\n",
    "        ),\n",
    "        input=pps_proto.Input(\n",
    "            cross=[\n",
    "                pps_proto.Input(\n",
    "                    pfs=pps_proto.Input(repo=\"dataset\", glob=\"/*\")\n",
    "                ),\n",
    "                pps_proto.Input(\n",
    "                    pfs=pps_proto.Input(repo=\"sentiment_words\", glob=\"/\")\n",
    "                )\n",
    "            ]),\n",
    "        update=True,\n",
    "        reprocess_spec=\"every_job\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8819420a-c767-4e6e-a972-ce77966bfc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_parallelized_python_pipeline('visualizations', client, code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec56161-55f3-4aac-bfaa-6437103d2b74",
   "metadata": {},
   "source": [
    "If we list our jobs, we can see che changes in the `PROGRESS` field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c090d2d3-83b1-4ad9-badb-1183b4511ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE       ID                               STARTED        DURATION   RESTART PROGRESS  DL       UL       STATE                                 \n",
      "visualizations 4261036ed5f64d1b8d7ee563961edf0b 4 minutes ago  27 seconds 0       3 + 0 / 3 913.8KiB 424.1KiB \u001b[32msuccess\u001b[0m                               \n",
      "visualizations 22fd16c90d7b47429c90ac5e5b8245cd 9 minutes ago  26 seconds 0       3 + 0 / 3 913.8KiB 419.5KiB \u001b[32msuccess\u001b[0m                               \n",
      "visualizations 4472d147797143ed88a40a3c67ae308a 16 minutes ago 16 seconds 0       1 + 0 / 1 750.7KiB 414.5KiB \u001b[32msuccess\u001b[0m                               \n",
      "visualizations ad348405004947ea9595d11f09273c49 20 minutes ago 3 seconds  0       0 + 0 / 1 750.7KiB 0B       \u001b[31mfailure\u001b[0m: datum 51932426570330879ff... \n",
      "visualizations d862e8fe11924198bcdfc604ec43b829 2 hours ago    10 seconds 0       1 + 0 / 1 750.7KiB 141.3KiB \u001b[32msuccess\u001b[0m                               \n"
     ]
    }
   ],
   "source": [
    "!pachctl list job -p visualizations --history all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de425b-6e5f-4cf5-baa3-0e15d72ff068",
   "metadata": {},
   "source": [
    "The format of the progress column is `DATUMS PROCESSED + DATUMS SKIPPED / TOTAL DATUMS`.\n",
    "We can see that our `PROGRESS`value has changed from `1 + 0 / 1` to `3 + 0 / 3`, meaning instead of executing everything in one job, without changing any code, Pachyderm can split our data and run our code on each piece independently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "883360b3-a729-4061-ad01-7b469450255d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE       ID                               STARTED     DURATION   RESTART PROGRESS  DL       UL       STATE   \n",
      "visualizations d862e8fe11924198bcdfc604ec43b829 2 hours ago 10 seconds 0       1 + 0 / 1 750.7KiB 141.3KiB \u001b[32msuccess\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "# Original job had a single datum \n",
    "!pachctl list job d862e8fe11924198bcdfc604ec43b829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "20db2a19-49ff-402e-b73c-225c9e2c5adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE       ID                               STARTED        DURATION   RESTART PROGRESS  DL       UL       STATE   \n",
      "visualizations 4261036ed5f64d1b8d7ee563961edf0b 28 seconds ago 27 seconds 0       3 + 0 / 3 913.8KiB 424.1KiB \u001b[32msuccess\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "# New job has 3 datums that can execute in parallel \n",
    "!pachctl list job 4261036ed5f64d1b8d7ee563961edf0b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22329f62-fc21-4619-8204-92bd6d727173",
   "metadata": {},
   "source": [
    "If we view the datums that were input to each job, we can see that our Pachyderm pipeline automatically paralleizes our code without us having to make any changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "345d4e65-6352-4717-b570-13ea08e05a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                                                               FILES                                                                                                        STATUS  TIME      \n",
      "22b0aef9fa4e54ff1e967e1c28716b5b55a49b90e75a396d3a4442d81a2d79b6 dataset@4261036ed5f64d1b8d7ee563961edf0b:/validation.csv, sentiment_words@4261036ed5f64d1b8d7ee563961edf0b:/ \u001b[32msuccess\u001b[0m 3 seconds \n",
      "41dae939561609fc88d8f72cdff08adaf6d9f6f1ea8c41809a448dac84d5404a dataset@4261036ed5f64d1b8d7ee563961edf0b:/train.csv, sentiment_words@4261036ed5f64d1b8d7ee563961edf0b:/      \u001b[32msuccess\u001b[0m 6 seconds \n",
      "9c92a8264bd805733271a310ce90761ed00cd92d3344410fe2ea175fcac54c78 dataset@4261036ed5f64d1b8d7ee563961edf0b:/test.csv, sentiment_words@4261036ed5f64d1b8d7ee563961edf0b:/       \u001b[32msuccess\u001b[0m 4 seconds \n"
     ]
    }
   ],
   "source": [
    "!pachctl list datum visualizations@4261036ed5f64d1b8d7ee563961edf0b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
