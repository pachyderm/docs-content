{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data-Centric Market Sentiment Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p align=\"center\">\n",
    "\t<img src='images/market_sentiment.png' width='1000' title=''>\n",
    "</p>\n",
    "\n",
    "In this example, we show a market sentiment NLP implementation in Pachyderm. In it, we use [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) to fine-tune a BERT language model to classify text for financial sentiment. It shows how to combine inputs from separate sources, incorporates data labeling, model training, and data visualization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Initial Setup\n",
    "Download the financial phrase bank and unzip it to the `data` directory.\n",
    "\n",
    "1. The Financial Phrase Bank Dataset should be [downloaded](https://www.researchgate.net/publication/251231364_FinancialPhraseBank-v10) and placed in `data/FinancialPhraseBank/`. (Due to data access permissions, this step must be done manually.)\n",
    "2. Unzip the dataset. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "!unzip data/FinancialPhraseBank-v1.0.zip -d data/"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Archive:  data/FinancialPhraseBank-v1.0.zip\n",
      "   creating: data/FinancialPhraseBank-v1.0/\n",
      "  inflating: data/FinancialPhraseBank-v1.0/License.txt  \n",
      "   creating: data/__MACOSX/\n",
      "   creating: data/__MACOSX/FinancialPhraseBank-v1.0/\n",
      "  inflating: data/__MACOSX/FinancialPhraseBank-v1.0/._License.txt  \n",
      "  inflating: data/FinancialPhraseBank-v1.0/README.txt  \n",
      "  inflating: data/__MACOSX/FinancialPhraseBank-v1.0/._README.txt  \n",
      "  inflating: data/FinancialPhraseBank-v1.0/Sentences_50Agree.txt  \n",
      "  inflating: data/FinancialPhraseBank-v1.0/Sentences_66Agree.txt  \n",
      "  inflating: data/FinancialPhraseBank-v1.0/Sentences_75Agree.txt  \n",
      "  inflating: data/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt  \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Download the pre-trained BERT language model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "!chmod +x download_model.sh"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "!./download_model.sh"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2021-09-21 19:31:04--  https://huggingface.co/ProsusAI/finbert/resolve/main/README.md\n",
      "Resolving huggingface.co (huggingface.co)... 107.23.77.87, 3.213.134.133, 34.195.144.223, ...\n",
      "Connecting to huggingface.co (huggingface.co)|107.23.77.87|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1475 (1.4K) [text/markdown]\n",
      "Saving to: ‘./models/finbertTCR2/README.md’\n",
      "\n",
      "README.md           100%[===================>]   1.44K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-09-21 19:31:05 (239 MB/s) - ‘./models/finbertTCR2/README.md’ saved [1475/1475]\n",
      "\n",
      "--2021-09-21 19:31:05--  https://huggingface.co/ProsusAI/finbert/resolve/main/config.json\n",
      "Resolving huggingface.co (huggingface.co)... 34.200.164.230, 34.195.144.223, 3.213.134.133, ...\n",
      "Connecting to huggingface.co (huggingface.co)|34.200.164.230|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 758 [application/json]\n",
      "Saving to: ‘./models/finbertTCR2/config.json’\n",
      "\n",
      "config.json         100%[===================>]     758  --.-KB/s    in 0s      \n",
      "\n",
      "2021-09-21 19:31:05 (127 MB/s) - ‘./models/finbertTCR2/config.json’ saved [758/758]\n",
      "\n",
      "--2021-09-21 19:31:05--  https://huggingface.co/ProsusAI/finbert/resolve/main/pytorch_model.bin\n",
      "Resolving huggingface.co (huggingface.co)... 107.23.77.87, 34.200.164.230, 34.195.144.223, ...\n",
      "Connecting to huggingface.co (huggingface.co)|107.23.77.87|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/ProsusAI/finbert/e15a7b5738df7f17553399b6d94c6e2ff69c89245d066e8e5d183f5803a554e3 [following]\n",
      "--2021-09-21 19:31:05--  https://cdn-lfs.huggingface.co/ProsusAI/finbert/e15a7b5738df7f17553399b6d94c6e2ff69c89245d066e8e5d183f5803a554e3\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 52.84.162.46, 52.84.162.9, 52.84.162.75, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|52.84.162.46|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 437992753 (418M) [application/octet-stream]\n",
      "Saving to: ‘./models/finbertTCR2/pytorch_model.bin’\n",
      "\n",
      "pytorch_model.bin   100%[===================>] 417.70M  75.4MB/s    in 7.0s    \n",
      "\n",
      "2021-09-21 19:31:13 (59.9 MB/s) - ‘./models/finbertTCR2/pytorch_model.bin’ saved [437992753/437992753]\n",
      "\n",
      "--2021-09-21 19:31:13--  https://huggingface.co/ProsusAI/finbert/resolve/main/special_tokens_map.json\n",
      "Resolving huggingface.co (huggingface.co)... 3.213.134.133, 107.23.77.87, 34.200.164.230, ...\n",
      "Connecting to huggingface.co (huggingface.co)|3.213.134.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 112 [application/json]\n",
      "Saving to: ‘./models/finbertTCR2/special_tokens_map.json’\n",
      "\n",
      "special_tokens_map. 100%[===================>]     112  --.-KB/s    in 0s      \n",
      "\n",
      "2021-09-21 19:31:13 (21.9 MB/s) - ‘./models/finbertTCR2/special_tokens_map.json’ saved [112/112]\n",
      "\n",
      "--2021-09-21 19:31:13--  https://huggingface.co/ProsusAI/finbert/resolve/main/tokenizer_config.json\n",
      "Resolving huggingface.co (huggingface.co)... 34.195.144.223, 3.213.134.133, 107.23.77.87, ...\n",
      "Connecting to huggingface.co (huggingface.co)|34.195.144.223|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 252 [application/json]\n",
      "Saving to: ‘./models/finbertTCR2/tokenizer_config.json’\n",
      "\n",
      "tokenizer_config.js 100%[===================>]     252  --.-KB/s    in 0s      \n",
      "\n",
      "2021-09-21 19:31:13 (43.9 MB/s) - ‘./models/finbertTCR2/tokenizer_config.json’ saved [252/252]\n",
      "\n",
      "--2021-09-21 19:31:13--  https://huggingface.co/ProsusAI/finbert/resolve/main/vocab.txt\n",
      "Resolving huggingface.co (huggingface.co)... 34.200.164.230, 34.195.144.223, 3.213.134.133, ...\n",
      "Connecting to huggingface.co (huggingface.co)|34.200.164.230|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 231508 (226K) [text/plain]\n",
      "Saving to: ‘./models/finbertTCR2/vocab.txt’\n",
      "\n",
      "vocab.txt           100%[===================>] 226.08K  1.11MB/s    in 0.2s    \n",
      "\n",
      "2021-09-21 19:31:14 (1.11 MB/s) - ‘./models/finbertTCR2/vocab.txt’ saved [231508/231508]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Set up data repos and push data\n",
    "\n",
    "For simplicity, we'll be using the Pachyderm command line utility (CLI) to run our commands. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first thing we’ll do in pachyderm is create a data repository to hold our sentiment dataset. \n",
    "Data repositories are the way that we organize our data in Pachyderm. \n",
    "\n",
    "Data repositories are similar to Git repositories in that they store data as commits. This means that I can upload a file, and if I ever overwrite it, I can go back to the previous version. However, unlike Git and most other versioning systems based on it, Pachyderm is built specifically for file storage, meaning that it can be used to track and version text data, images, audio, models, and anything else that can be treated as a file.\n",
    "\n",
    "The 50 Agree means that I’m using the version of the dataset where there was at least 50% agreement between labelers. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Upload the Financial Phrase Bank data\n",
    "!pachctl create repo financial_phrase_bank\n",
    "!pachctl put file financial_phrase_bank@master:/Sentences_50Agree.txt -f data/FinancialPhraseBank-v1.0/Sentences_50Agree.txt"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data/FinancialPhraseBank-v1.0/Sentences_50Agree.txt 671.36 KB / 671.36 KB  0s 0…\n",
      "\u001b[1A\u001b[Jdata/FinancialPhraseBank-v1.0/Sentences_50Agree.txt 671.36 KB / 671.36 KB  0s 0…\n",
      "\u001b[1A\u001b[Jdata/FinancialPhraseBank-v1.0/Sentences_50Agree.txt 671.36 KB / 671.36 KB  0s 0…\n",
      "\u001b[1A\u001b[Jdata/FinancialPhraseBank-v1.0/Sentences_50Agree.txt 671.36 KB / 671.36 KB  0s 0…\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll also set up a repository to hold my labeled data for later on in the demo. I’m also creating an empty commit here, which essentially is just telling pachyderm I’m not putting any data in this repo now, but I still want it to be processed. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Upload the language model to Pachyderm\n",
    "!pachctl create repo language_model\n",
    "!cd models/finbertTCR2/; pachctl put file -r language_model@master -f ./; cd ../../"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "README.md 1.48 KB / 1.48 KB [======================================] 0s 0.00 b/s\n",
      "\u001b[1A\u001b[JREADME.md 1.48 KB / 1.48 KB [======================================] 0s 0.00 b/s\n",
      "config.json 0.00 b / 758.00 b [------------------------------------] 0s 0.00 b/s\n",
      "\u001b[2A\u001b[JREADME.md 1.48 KB / 1.48 KB [======================================] 0s 0.00 b/s\n",
      "config.json 758.00 b / 758.00 b [==================================] 0s 0.00 b/s\n",
      "\u001b[2A\u001b[JREADME.md 1.48 KB / 1.48 KB [======================================] 0s 0.00 b/s\n",
      "config.json 758.00 b / 758.00 b [==================================] 0s 0.00 b/s\n",
      "pytorch_model.bin 0.00 b / 437.99 MB [-----------------------------] 0s 0.00 b/s\n",
      "\u001b[3A\u001b[JREADME.md 1.48 KB / 1.48 KB [======================================] 0s 0.00 b/s\n",
      "config.json 758.00 b / 758.00 b [==================================] 0s 0.00 b/s\n",
      "pytorch_model.bin 2.10 MB / 437.99 MB [----------------------------] 0s 0.00 b/s\n",
      "\u001b[3A\u001b[JREADME.md 1.48 KB / 1.48 KB [======================================] 0s 0.00 b/s\n",
      "config.json 758.00 b / 758.00 b [==================================] 0s 0.00 b/s\n",
      "pytorch_model.bin 10.49 MB / 437.99 MB [>--------------------------] 0s 0.00 b/s\n",
      "\u001b[3A\u001b[JREADME.md 1.48 KB / 1.48 KB [======================================] 0s 0.00 b/s\n",
      "config.json 758.00 b / 758.00 b [==================================] 0s 0.00 b/s\n",
      "pytorch_model.bin 27.26 MB / 437.99 MB [=>-----------------------] 4s 95.02 MB/s\n",
      "\u001b[3A\u001b[JREADME.md 1.48 KB / 1.48 KB [======================================] 0s 0.00 b/s\n",
      "config.json 758.00 b / 758.00 b [==================================] 0s 0.00 b/s\n",
      "pytorch_model.bin 41.94 MB / 437.99 MB [=>----------------------] 3s 102.40 MB/s\n",
      "\u001b[3A\u001b[JREADME.md 1.48 KB / 1.48 KB [======================================] 0s 0.00 b/s\n",
      "config.json 758.00 b / 758.00 b [==================================] 0s 0.00 b/s\n",
      "pytorch_model.bin 62.91 MB / 437.99 MB [==>---------------------] 3s 122.25 MB/s\n",
      "\u001b[3A\u001b[JREADME.md 1.48 KB / 1.48 KB [======================================] 0s 0.00 b/s\n",
      "config.json 758.00 b / 758.00 b [==================================] 0s 0.00 b/s\n",
      "pytorch_model.bin 73.40 MB / 437.99 MB [===>--------------------] 3s 117.49 MB/s\n",
      "\u001b[3A\u001b[JREADME.md 1.48 KB / 1.48 KB [======================================] 0s 0.00 b/s\n",
      "config.json 758.00 b / 758.00 b [==================================] 0s 0.00 b/s\n",
      "pytorch_model.bin 92.27 MB / 437.99 MB [====>-------------------] 2s 128.16 MB/s\n",
      "\u001b[3A\u001b[JREADME.md 1.48 KB / 1.48 KB [======================================] 0s 0.00 b/s\n",
      "config.json 758.00 b / 758.00 b [==================================] 0s 0.00 b/s\n",
      "special_tokens_map.json 112.00 b / 112.00 b [======================] 0s 0.00 b/s\n",
      "tokenizer_config.json 252.00 b / 252.00 b [========================] 0s 0.00 b/s\n",
      "vocab.txt 231.51 KB / 231.51 KB [==================================] 0s 0.00 b/s\n",
      "\u001b[6A\u001b[JREADME.md 1.48 KB / 1.48 KB [======================================] 0s 0.00 b/s\n",
      "config.json 758.00 b / 758.00 b [==================================] 0s 0.00 b/s\n",
      "pytorch_model.bin 437.99 MB / 437.99 MB [=======================] 0s 124.38 MB/s\n",
      "special_tokens_map.json 112.00 b / 112.00 b [======================] 0s 0.00 b/s\n",
      "tokenizer_config.json 252.00 b / 252.00 b [========================] 0s 0.00 b/s\n",
      "vocab.txt 231.51 KB / 231.51 KB [==================================] 0s 0.00 b/s\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Set up labeled_data repo for labeling production data later\n",
    "!pachctl create repo labeled_data\n",
    "!pachctl start commit labeled_data@master; pachctl finish commit labeled_data@master"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "e493e1e8c2c443f392092b66e7f2807f\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: Here we create a commit in the `labeled_data` repo with an empty file as a place holder. This allows our pipeline to run without having to have labeled production data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Deploy Pachyderm Pipelines "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now I can deploy my dataset creation and model training pipeline using `pachctl create pipeline`. This pipeline will combine my two data sources, create training, testing, and validation sets, and then write them to its output repo. This repository is versioned as well, which means that I can revert back to any dataset version that was created by my pipeline. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Deploy the dataset creation pipeline\n",
    "!pachctl create pipeline -f pachyderm/dataset.json\n",
    "\n",
    "# Deploy the training pipeline\n",
    "!pachctl create pipeline -f pachyderm/train_model.json"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at the pipeline to see what’s actually happening under the hood. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "!cat pachyderm/dataset.json"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n",
      "  \"pipeline\": {\n",
      "    \"name\": \"dataset\"\n",
      "  },\n",
      "  \"description\": \"Create an FPB formatted dataset for labeled text data.\",\n",
      "  \"input\": {\n",
      "    \"join\": [\n",
      "      {\n",
      "          \"pfs\": {\n",
      "              \"glob\": \"/\",\n",
      "              \"repo\": \"labeled_data\",\n",
      "              \"outer_join\": true\n",
      "          }\n",
      "      },\n",
      "      {\n",
      "          \"pfs\": {\n",
      "              \"glob\": \"/\",\n",
      "              \"repo\": \"financial_phrase_bank\",\n",
      "              \"outer_join\": true\n",
      "          }\n",
      "      }\n",
      "  ]\n",
      "  },\n",
      "  \"transform\": {\n",
      "    \"cmd\": [\n",
      "      \"python\", \"completions-dataset.py\",\n",
      "      \"--completions-dir\", \"/pfs/labeled_data/\",\n",
      "      \"--fpb-dataset\", \"/pfs/financial_phrase_bank/\",\n",
      "      \"--output-dir\", \"/pfs/out/\"\n",
      "    ],\n",
      "    \"image\": \"jimmywhitaker/market_sentiment:dev0.25\"\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We won't go through all of the details of this pipeline, but the key componets are: \n",
    "* Input  - data repositories that will be mapping the pipeline when it runs. They will available through the file system at `/pfs/`\n",
    "* Transform - shows the Docker image and the python command that will process our data. \n",
    "\n",
    "The output will be stored in `/pfs/out` which will version all the files there automatically when the container is finished. So if I overwrite my dataset, then I can still access it by viewing a previous commit. \n",
    "\n",
    "Notice also, that I just created my pipelines, but I never had to tell them to run. All pipelines in Pachyderm are data driven, meaning that changes in the data repositories are what control the processing flow. \n",
    "\n",
    "This means that I can start experimenting with different versions of datasets or pipelines and never lose anything! \n",
    "\n",
    "**Note**: \n",
    "\n",
    "[Glob patterns](https://docs.pachyderm.com/latest/concepts/pipeline-concepts/datum/glob-pattern/) on our inputs are also very powerful. You can tell a pipeline how to split your data up so that you can parallelize your pipeline with no extra code, which is very useful in data cleaning and preprocessing. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. (Optional) Deploy Data Visualization Pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Use a sentiment word list to visualize the current dataset\n",
    "!pachctl create repo sentiment_words\n",
    "!pachctl put file sentiment_words@master:/LoughranMcDonald_SentimentWordLists_2018.csv -f resources/LoughranMcDonald_SentimentWordLists_2018.csv\n",
    "!pachctl create pipeline -f pachyderm/visualizations.json"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "resources/LoughranMcDonald_SentimentWordLists_2018.csv 83.53 KB / 83.53 KB  0s …\n",
      "\u001b[1A\u001b[Jresources/LoughranMcDonald_SentimentWordLists_2018.csv 83.53 KB / 83.53 KB  0s …\n",
      "\u001b[1A\u001b[Jresources/LoughranMcDonald_SentimentWordLists_2018.csv 83.53 KB / 83.53 KB  0s …\n",
      "\u001b[1A\u001b[Jresources/LoughranMcDonald_SentimentWordLists_2018.csv 83.53 KB / 83.53 KB  0s …\n",
      "\u001b[1A\u001b[Jresources/LoughranMcDonald_SentimentWordLists_2018.csv 83.53 KB / 83.53 KB  0s …\n",
      "\u001b[1A\u001b[Jresources/LoughranMcDonald_SentimentWordLists_2018.csv 83.53 KB / 83.53 KB  0s …\n",
      "\u001b[1A\u001b[Jresources/LoughranMcDonald_SentimentWordLists_2018.csv 83.53 KB / 83.53 KB  0s …\n",
      "\u001b[1A\u001b[Jresources/LoughranMcDonald_SentimentWordLists_2018.csv 83.53 KB / 83.53 KB  0s …\n",
      "\u001b[1A\u001b[Jresources/LoughranMcDonald_SentimentWordLists_2018.csv 83.53 KB / 83.53 KB  0s …\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Update Our Dataset (Data-Driven Pipelines)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Say I wanted to change the version of the Financial Phrase Bank. \n",
    "\n",
    "I originally uploaded the one where 50% of the labelers agreed on the prediction, but maybe that’s resulting in a low model accuracy. Let’s change that to one where all labelers agree. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we'll tag the version of data that was used in this training job as `v1`. \n",
    "\n",
    "Note: Pachyderm will track all versions that are created, but this will make it easier to refer to this branch later on."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "!pachctl create branch financial_phrase_bank@v1 --head master"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "!pachctl list commit financial_phrase_bank@v1\n",
    "!pachctl list file financial_phrase_bank@v1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "REPO                  BRANCH COMMIT                           FINISHED      SIZE     ORIGIN DESCRIPTION\n",
      "financial_phrase_bank master 37ecb36189bb4817a65aa45405eac122 4 minutes ago 655.6KiB USER    \n",
      "NAME                   TYPE SIZE     \n",
      "/Sentences_50Agree.txt file 655.6KiB \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Modify the version of Financial Phrase Bank dataset used\n",
    "!pachctl start commit financial_phrase_bank@master\n",
    "!pachctl delete file financial_phrase_bank@master:/Sentences_50Agree.txt\n",
    "!pachctl put file financial_phrase_bank@master:/Sentences_AllAgree.txt -f data/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\n",
    "!pachctl finish commit financial_phrase_bank@master"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a3b82817b3c24633ad9b7a273e618fe4\n",
      "data/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt 299.64 KB / 299.64 KB  0s …\n",
      "\u001b[1A\u001b[Jdata/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt 299.64 KB / 299.64 KB  0s …\n",
      "\u001b[1A\u001b[Jdata/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt 299.64 KB / 299.64 KB  0s …\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "!pachctl list commit financial_phrase_bank@v1\n",
    "!pachctl list file financial_phrase_bank@v1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "REPO                  BRANCH COMMIT                           FINISHED      SIZE     ORIGIN DESCRIPTION\n",
      "financial_phrase_bank master 37ecb36189bb4817a65aa45405eac122 5 minutes ago 655.6KiB USER    \n",
      "NAME                   TYPE SIZE     \n",
      "/Sentences_50Agree.txt file 655.6KiB \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "!pachctl list commit financial_phrase_bank@master\n",
    "!pachctl list file financial_phrase_bank@master"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "REPO                  BRANCH COMMIT                           FINISHED       SIZE     ORIGIN DESCRIPTION\n",
      "financial_phrase_bank master a3b82817b3c24633ad9b7a273e618fe4 34 seconds ago 292.6KiB USER    \n",
      "financial_phrase_bank master 37ecb36189bb4817a65aa45405eac122 5 minutes ago  655.6KiB USER    \n",
      "NAME                    TYPE SIZE     \n",
      "/Sentences_AllAgree.txt file 292.6KiB \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now this is cool: I don’t have to re-run my pipelines. Pachyderm recognizes the changes to the pipeline’s input, and it automatically kicks off my new training job. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Version our new dataset\n",
    "!pachctl create branch financial_phrase_bank@v2 --head master"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This gives you iron-clad reproducibility and lineage for anything that is put into pachyderm. Ensuring that you never lose anything and know how everything is connected. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "!pachctl list job --pipeline=train_model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PIPELINE    ID                               STARTED        DURATION       RESTART PROGRESS  DL       UL       STATE   \n",
      "train_model a3b82817b3c24633ad9b7a273e618fe4 12 minutes ago About a minute 0       1 + 0 / 1 418.2MiB 417.7MiB \u001b[32msuccess\u001b[0m \n",
      "train_model 42e4ec6252c64a55b049c6e45eb99a59 15 minutes ago 2 minutes      0       1 + 0 / 1 418.6MiB 417.7MiB \u001b[32msuccess\u001b[0m \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Add Labeled Production Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we'll create a data repositor that will hold all of our raw data that comes from production. \n",
    "\n",
    "We will connect our labeling tool to this repo and write our labeled data to the `labeled_data` repo. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "!pachctl create repo raw_data\n",
    "!pachctl put file raw_data@master:/example1.json -f data/round1/example1.json"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data/round1/example1.json 623.00 b / 623.00 b [====================] 0s 0.00 b/s\n",
      "\u001b[1A\u001b[Jdata/round1/example1.json 623.00 b / 623.00 b [====================] 0s 0.00 b/s\n",
      "\u001b[1A\u001b[Jdata/round1/example1.json 623.00 b / 623.00 b [====================] 0s 0.00 b/s\n",
      "\u001b[1A\u001b[Jdata/round1/example1.json 623.00 b / 623.00 b [====================] 0s 0.00 b/s\n",
      "\u001b[1A\u001b[Jdata/round1/example1.json 623.00 b / 623.00 b [====================] 0s 0.00 b/s\n",
      "\u001b[1A\u001b[Jdata/round1/example1.json 623.00 b / 623.00 b [====================] 0s 0.00 b/s\n",
      "\u001b[1A\u001b[Jdata/round1/example1.json 623.00 b / 623.00 b [====================] 0s 0.00 b/s\n",
      "\u001b[1A\u001b[Jdata/round1/example1.json 623.00 b / 623.00 b [====================] 0s 0.00 b/s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the [Label Studio Integration](https://github.com/pachyderm/examples/tree/master/label-studio) to connect a labeling envorinment with data versioned in Pachyderm. \n",
    "\n",
    "<p align=\"center\">\n",
    "\t<img src='images/label_studio_screenshot.png' width='1000' title=''>\n",
    "</p>\n",
    "\n",
    "As soon as we label an example, it will automatically kick off a retraining of our model. \n",
    "\n",
    "\n",
    "Alternatively, if running this notebook without Label Studio, you can place the data directly in the labeled data repository, by uncommenting and running the following cell. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!pachctl put file labeled_data@master:/example1_labeled.json -f data/round1/example1_labeled.json"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
